# Start from Alpaca

Novices also want to join the bandwagon of building LLMs !

Start from Organizing and assembling the Existing repository, and then try to add something new. 

test


## Source

### CheckPointsï¼š

- [LLaMA](https://huggingface.co/decapoda-research)ï¼šThe original Weights

- [Alpaca-native](https://huggingface.co/chavinlo/alpaca-native)ï¼š Using the origin code from  Stanford-Alpaca without lora

- [tloen/alpaca-lora-7b](https://huggingface.co/tloen/alpaca-lora-7b): the original 7B Alpaca-LoRA checkpoint by tloen

- [chansung/alpaca-lora-13b](https://huggingface.co/chansung/alpaca-lora-13b): the 13B Alpaca-LoRA checkpoint by changsung with the same script to tune the original 7B model

- [chansung/alpaca-lora-30b](https://huggingface.co/chansung/alpaca-lora-30b): the 30B Alpaca-LoRA checkpoint by chansung with the same script to tune the original 7B model

- ä¸­æ–‡LLaMAæ¨¡å‹åœ¨åŸç‰ˆçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œä½¿ç”¨äº†ä¸­æ–‡çº¯æ–‡æœ¬æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œå…·ä½“è§[è®­ç»ƒç»†èŠ‚](https://github.com/ymcui/Chinese-LLaMA-Alpaca#è®­ç»ƒç»†èŠ‚)ä¸€èŠ‚ã€‚
  **æ³¨æ„ï¼šå¦‚å¸Œæœ›ä½“éªŒç±»ChatGPTå¯¹è¯äº¤äº’ï¼Œè¯·ä½¿ç”¨Alpacaæ¨¡å‹ï¼Œè€Œä¸æ˜¯LLaMAæ¨¡å‹ã€‚**

  | æ¨¡å‹åç§°           | ç±»å‹     | é‡æ„æ‰€éœ€æ¨¡å‹     | å¤§å°[2] | LoRAä¸‹è½½åœ°å€                                                 | SHA256[3]          |
  | ------------------ | -------- | ---------------- | ------- | ------------------------------------------------------------ | ------------------ |
  | Chinese-LLaMA-7B   | é€šç”¨     | åŸç‰ˆLLaMA-7B[1]  | 770M    | [[ç™¾åº¦ç½‘ç›˜\]](https://pan.baidu.com/s/1oORTdpr2TvlkxjpyWtb5Sw?pwd=33hb) [[Google Drive\]](https://drive.google.com/file/d/1iQp9T-BHjBjIrFWXq_kIm_cyNmpvv5WN/view?usp=sharing) [[HuggingFace\]](https://huggingface.co/ziqingyang/chinese-llama-lora-7b) | 39b86b......fe0e60 |
  | Chinese-LLaMA-13B  | é€šç”¨     | åŸç‰ˆLLaMA-13B[1] | â³       | â³                                                            | â³                  |
  | Chinese-Alpaca-7B  | æŒ‡ä»¤ç²¾è°ƒ | åŸç‰ˆLLaMA-7B[1]  | 790M    | [[ç™¾åº¦ç½‘ç›˜\]](https://pan.baidu.com/s/1xV1UXjh1EPrPtXg6WyG7XQ?pwd=923e) [[Google Drive\]](https://drive.google.com/file/d/1JvFhBpekYiueWiUL3AF1TtaWDb3clY5D/view?usp=sharing) [[HuggingFace\]](https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b) | 9bb5b6......ce2d87 |
  | Chinese-Alpaca-13B | æŒ‡ä»¤ç²¾è°ƒ | åŸç‰ˆLLaMA-13B[1] | â³       | â³                                                            | â³                  |



### Building the model:

[Stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

ã€The :one:first popular player using the LLaMAã€‘This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:

- The [52K data](https://github.com/tatsu-lab/stanford_alpaca#data-release) used for fine-tuning the model.
- The code for [generating the data](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process).
- The code for [fine-tuning the model](https://github.com/tatsu-lab/stanford_alpaca#fine-tuning).



[Alpaca-lora](https://github.com/tloen/alpaca-lora)

This repository contains code for reproducing the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) results using [low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf). We provide an Instruct model of similar quality to `text-davinci-003` that can run [on a Raspberry Pi](https://twitter.com/miolini/status/1634982361757790209) (for research), and the code is easily extended to the `13b`, `30b`, and `65b` models.

In addition to the training code, which runs within hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting [LoRA weights themselves](https://huggingface.co/tloen/alpaca-lora-7b/tree/main). To fine-tune cheaply and efficiently, we use Hugging Face's [PEFT](https://github.com/huggingface/peft) as well as Tim Dettmers' [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).

Without hyperparameter tuning, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance



**[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)**

æœ¬é¡¹ç›®å¼€æºäº†**ä¸­æ–‡LLaMAæ¨¡å‹å’Œç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹**ã€‚è¿™äº›æ¨¡å‹**åœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨**å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œåœ¨ä¸­æ–‡LLaMAçš„åŸºç¡€ä¸Šï¼Œæœ¬é¡¹ç›®ä½¿ç”¨äº†ä¸­æ–‡æŒ‡ä»¤æ•°æ®è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚

**ä¸»è¦å†…å®¹ï¼š**

ğŸš€ å¼€æºäº†ç»è¿‡ä¸­æ–‡æ–‡æœ¬æ•°æ®é¢„è®­ç»ƒçš„ä¸­æ–‡LLaMAå¤§æ¨¡å‹

ğŸš€ å¼€æºäº†è¿›ä¸€æ­¥ç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ä¸­æ–‡Alpacaå¤§æ¨¡å‹

ğŸš€ å¿«é€Ÿåœ°ä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆä¸ªäººPCï¼‰æœ¬åœ°éƒ¨ç½²å’Œä½“éªŒé‡åŒ–ç‰ˆå¤§æ¨¡å‹



**[llama.cpp](https://github.com/ggerganov/llama.cpp)**

Inference of [LLaMA](https://arxiv.org/abs/2302.13971) model in pure C/C++ :weary:

The main goal is to run the model using 4-bit quantization on a MacBook(also supports Windows)

- Plain C/C++ implementation without dependencies
- Apple silicon first-class citizen - optimized via ARM NEON and Accelerate framework
- AVX2 support for x86 architectures
- Mixed F16 / F32 precision
- 4-bit quantization support
- Runs on the CPU



###  Data && Deployment

**[AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned)**

This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.



[Alpaca-LoRA-Serve](https://github.com/deep-diver/Alpaca-LoRA-Serve)

Alpaca-LoRA as a Chatbot Service

<img src="https://cdn.jsdelivr.net/gh/Darren-greenhand/Darren-greenhand-image@main/img/202304011729858.png" alt="img" style="zoom: 19%;" />



**[Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp)**

Run a fast ChatGPT-like model locally on your device.

Based on the [Alpaca-lora](https://github.com/tloen/alpaca-lora) and the horiible [llama.cpp](https://github.com/ggerganov/llama.cpp) but  add a **chat interface**.





## å…è´£å£°æ˜

æœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›å­¦æœ¯ç ”ç©¶ä¹‹ç”¨ï¼Œä¸¥ç¦ç”¨äºå•†ä¸šç”¨é€”ã€‚ä½¿ç”¨æ¶‰åŠç¬¬ä¸‰æ–¹ä»£ç çš„éƒ¨åˆ†æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªç›¸åº”çš„å¼€æºåè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å—æ¨¡å‹è®¡ç®—ã€éšæœºæ€§å’Œé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®æ— æ³•å¯¹å…¶å‡†ç¡®æ€§ä½œå‡ºä¿è¯ã€‚å¯¹äºæ¨¡å‹è¾“å‡ºçš„ä»»ä½•å†…å®¹ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚

æœ¬é¡¹ç›®ç”±ä¸ªäººåŠåä½œè€…ä¸šä½™æ—¶é—´å‘èµ·å¹¶ç»´æŠ¤ï¼Œå› æ­¤æ— æ³•ä¿è¯èƒ½åŠæ—¶å›å¤è§£å†³ç›¸åº”é—®é¢˜ã€‚
