# Start from Alpaca

Novices also want to join the bandwagon of building LLMs !

Start from Organizing and assembling the Existing repository, and then try to add something new !!


## Source

### CheckPointsï¼š

- [LLaMA](https://huggingface.co/decapoda-research)ï¼šThe original Weights

- [Alpaca-native](https://huggingface.co/chavinlo/alpaca-native)ï¼š Using the origin code from  Stanford-Alpaca without lora

- [tloen/alpaca-lora-7b](https://huggingface.co/tloen/alpaca-lora-7b): the original 7B Alpaca-LoRA checkpoint by tloen

- [chansung/alpaca-lora-13b](https://huggingface.co/chansung/alpaca-lora-13b): the 13B Alpaca-LoRA checkpoint by changsung with the same script to tune the original 7B model

- [chansung/alpaca-lora-30b](https://huggingface.co/chansung/alpaca-lora-30b): the 30B Alpaca-LoRA checkpoint by chansung with the same script to tune the original 7B model

- ä¸­æ–‡LLaMAæ¨¡å‹åœ¨åŸç‰ˆçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œä½¿ç”¨äº†ä¸­æ–‡çº¯æ–‡æœ¬æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œå…·ä½“è§[è®­ç»ƒç»†èŠ‚](https://github.com/ymcui/Chinese-LLaMA-Alpaca#è®­ç»ƒç»†èŠ‚)ä¸€èŠ‚ã€‚
  **æ³¨æ„ï¼šå¦‚å¸Œæœ›ä½“éªŒç±»ChatGPTå¯¹è¯äº¤äº’ï¼Œè¯·ä½¿ç”¨Alpacaæ¨¡å‹ï¼Œè€Œä¸æ˜¯LLaMAæ¨¡å‹ã€‚**

  | æ¨¡å‹åç§°           | ç±»å‹     | é‡æ„æ‰€éœ€æ¨¡å‹     | å¤§å°[2] | LoRAä¸‹è½½åœ°å€                                                 | SHA256[3]          |
  | ------------------ | -------- | ---------------- | ------- | ------------------------------------------------------------ | ------------------ |
  | Chinese-LLaMA-7B   | é€šç”¨     | åŸç‰ˆLLaMA-7B[1]  | 770M    | [[ç™¾åº¦ç½‘ç›˜\]](https://pan.baidu.com/s/1oORTdpr2TvlkxjpyWtb5Sw?pwd=33hb) [[Google Drive\]](https://drive.google.com/file/d/1iQp9T-BHjBjIrFWXq_kIm_cyNmpvv5WN/view?usp=sharing) [[HuggingFace\]](https://huggingface.co/ziqingyang/chinese-llama-lora-7b) | 39b86b......fe0e60 |
  | Chinese-LLaMA-13B  | é€šç”¨     | åŸç‰ˆLLaMA-13B[1] | â³       | â³                                                            | â³                  |
  | Chinese-Alpaca-7B  | æŒ‡ä»¤ç²¾è°ƒ | åŸç‰ˆLLaMA-7B[1]  | 790M    | [[ç™¾åº¦ç½‘ç›˜\]](https://pan.baidu.com/s/1xV1UXjh1EPrPtXg6WyG7XQ?pwd=923e) [[Google Drive\]](https://drive.google.com/file/d/1JvFhBpekYiueWiUL3AF1TtaWDb3clY5D/view?usp=sharing) [[HuggingFace\]](https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b) | 9bb5b6......ce2d87 |
  | Chinese-Alpaca-13B | æŒ‡ä»¤ç²¾è°ƒ | åŸç‰ˆLLaMA-13B[1] | â³       | â³                                                            | â³                  |
  
- Various adapter weights (download at own risk):
  
  - 7B:
    - https://huggingface.co/tloen/alpaca-lora-7b
    - https://huggingface.co/samwit/alpaca7B-lora
    - ğŸ‡§ğŸ‡· https://huggingface.co/22h/cabrita-lora-v0-1
    - ğŸ‡¨ğŸ‡³ https://huggingface.co/qychen/luotuo-lora-7b-0.1
    - ğŸ‡¯ğŸ‡µ https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0
    - ğŸ‡«ğŸ‡· https://huggingface.co/bofenghuang/vigogne-lora-7b
    - ğŸ‡¹ğŸ‡­ https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1
    - ğŸ‡©ğŸ‡ª https://huggingface.co/thisserand/alpaca_lora_german
    - ğŸ‡®ğŸ‡¹ https://huggingface.co/teelinsan/camoscio-7b-llama
    - ğŸ‡·ğŸ‡º https://huggingface.co/IlyaGusev/llama_7b_ru_turbo_alpaca_lora
  - 13B:
    - https://huggingface.co/chansung/alpaca-lora-13b
    - https://huggingface.co/mattreid/alpaca-lora-13b
    - https://huggingface.co/samwit/alpaca13B-lora
    - ğŸ‡¯ğŸ‡µ https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0
    - ğŸ‡°ğŸ‡· https://huggingface.co/chansung/koalpaca-lora-13b
    - ğŸ‡¨ğŸ‡³ https://huggingface.co/facat/alpaca-lora-cn-13b
    - ğŸ‡ªğŸ‡¸ https://huggingface.co/plncmm/guanaco-lora-13b
  - 30B:
    - https://huggingface.co/baseten/alpaca-30b
    - https://huggingface.co/chansung/alpaca-lora-30b
    - ğŸ‡¯ğŸ‡µ https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0



### Building the model:

[Stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

ã€The :one:first popular player using the LLaMAã€‘This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:

- The [52K data](https://github.com/tatsu-lab/stanford_alpaca#data-release) used for fine-tuning the model.
- The code for [generating the data](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process).
- The code for [fine-tuning the model](https://github.com/tatsu-lab/stanford_alpaca#fine-tuning).



[Alpaca-lora](https://github.com/tloen/alpaca-lora)

This repository contains code for reproducing the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) results using [low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf). We provide an Instruct model of similar quality to `text-davinci-003` that can run [on a Raspberry Pi](https://twitter.com/miolini/status/1634982361757790209) (for research), and the code is easily extended to the `13b`, `30b`, and `65b` models.

In addition to the training code, which runs within hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting [LoRA weights themselves](https://huggingface.co/tloen/alpaca-lora-7b/tree/main). To fine-tune cheaply and efficiently, we use Hugging Face's [PEFT](https://github.com/huggingface/peft) as well as Tim Dettmers' [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).

Without hyperparameter tuning, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance



[BELLE: Be Everyone's Large Language model Engine](https://github.com/LianjiaTech/BELLE)

é¡¹ç›®åŒ…å«ä»¥ä¸‹å†…å®¹:

é¡¹ç›®åŒ…å«ä»¥ä¸‹å†…å®¹:

- ![Docs](https://camo.githubusercontent.com/891a55bda5f44b5da6bb2a00163b7155fac2bd60154f59accc244b6c31783316/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f254538254145254144254537254242253833254534254242254133254537254130253831747261696e2d626c7565)

  - è¯¦è§[BELLE/train](https://github.com/LianjiaTech/BELLE/tree/main/train)ï¼Œå°½å¯èƒ½ç®€åŒ–çš„ä¸€ä¸ªè®­ç»ƒä»£ç å®ç°ï¼Œæ”¯æŒfinetuneï¼Œloraï¼Œdeepspeed

- ![Docs](https://camo.githubusercontent.com/411b860c93c9780ebbc8e876d0fea0f0fb9aeeb8e94c8bab8f850b28a2a3dab2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f254536253935254230254536253844254145254535254243253830254536253934254245312e354d2d626c7565)

   

  ![Docs](https://camo.githubusercontent.com/8a9ab3975a7baa6652c8618f95ed2fec580b07ba80ac11db1973050d52be8cf8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25453625393525423025453625384425414525453525424325383025453625393425424531304d2d626c7565)

  - è¯¦è§[BELLE/1.5M](https://github.com/LianjiaTech/BELLE/tree/main/1.5M)ï¼Œå‚è€ƒ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ç”Ÿæˆçš„ä¸­æ–‡æ•°æ®é›†[1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)ï¼›
  - æŒç»­å¼€æ”¾çš„æ•°æ®é›†ï¼Œè¯¦è§[BELLE/10M](https://github.com/LianjiaTech/BELLE/tree/main/10M)ï¼Œç›®å‰å¼€æ”¾äº†[0.25Mæ•°å­¦æŒ‡ä»¤æ•°æ®é›†](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)å’Œ[0.8Må¤šè½®ä»»åŠ¡å¯¹è¯æ•°æ®é›†](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)

- ![Docs](https://camo.githubusercontent.com/6baec3b647d7b89723332b80b62bef1e303ef430e41f75b0c977c72032288903/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362541382541312545352539452538422d626c7565)

  - åŸºäºBLOOMZ-7B1-mtä¼˜åŒ–åçš„æ¨¡å‹ï¼š[BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M)ï¼Œ[BELLE-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-7B-0.6M)ï¼Œ[BELLE-7B-1M](https://huggingface.co/BelleGroup/BELLE-7B-1M)ï¼Œ[BELLE-7B-2M](https://huggingface.co/BelleGroup/BELLE-7B-2M)
  - åŸºäº[huggingfaceçš„LLaMAå®ä¾‹](https://huggingface.co/decapoda-research)å®ç°[è°ƒä¼˜çš„æ¨¡å‹](https://huggingface.co/BelleGroup)ï¼š[BELLE-LLAMA-13B-2M](https://huggingface.co/BelleGroup/BELLE-LLAMA-13B-2M)ï¼Œ[BELLE-LLAMA-7B-2M](https://huggingface.co/BelleGroup/BELLE-LLAMA-7B-2M)ï¼Œ[BELLE-LLAMA-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-LLAMA-7B-0.6M)ã€‚è¯·æ³¨æ„ï¼Œæœ¬é¡¹ç›®ä¸èƒ½ä¿è¯å…¶æ˜¯åŸç‰ˆçš„LLaMAæ¨¡å‹ï¼Œä¹Ÿä¸èƒ½ä¿è¯è°ƒä¼˜åçš„æ¨¡å‹å’ŒLLaMAåŸç‰ˆæ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€‚è¯·å‚è€ƒ[Meta LLaMAçš„License](https://github.com/facebookresearch/llama/blob/main/LICENSE)å’Œ[huggingfaceçš„LLaMAå®ä¾‹çš„License](https://huggingface.co/decapoda-research/llama-7b-hf/blob/main/LICENSE)ï¼Œç›®å‰ä»…ä¾›å­¦ä¹ äº¤æµã€‚è¯·ä¸¥éµå®ˆLLaMAçš„ä½¿ç”¨é™åˆ¶ã€‚å¼ºçƒˆå»ºè®®å¤§å®¶åŸºäºè®­ç»ƒè„šæœ¬å’Œå¼€æ”¾æ•°æ®è°ƒä¼˜æ¨¡å‹ã€‚

- ![Docs](https://camo.githubusercontent.com/18346c77c508307e601aa65b46ad01f2435bb015b1291b6083feb313d678300f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f254536254138254131254535253945253842254539253837253846254535253843253936677074712d626c7565)

  - è¯¦è§[BELLE/gptq](https://github.com/LianjiaTech/BELLE/tree/main/gptq)ï¼Œå‚è€ƒgptqçš„å®ç°ï¼Œå¯¹æœ¬é¡¹ç›®ä¸­ç›¸å…³æ¨¡å‹è¿›è¡Œäº†é‡åŒ–



**[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)**

æœ¬é¡¹ç›®å¼€æºäº†**ä¸­æ–‡LLaMAæ¨¡å‹å’Œç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹**ã€‚è¿™äº›æ¨¡å‹**åœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨**å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œåœ¨ä¸­æ–‡LLaMAçš„åŸºç¡€ä¸Šï¼Œæœ¬é¡¹ç›®ä½¿ç”¨äº†ä¸­æ–‡æŒ‡ä»¤æ•°æ®è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚

**ä¸»è¦å†…å®¹ï¼š**

ğŸš€ å¼€æºäº†ç»è¿‡ä¸­æ–‡æ–‡æœ¬æ•°æ®é¢„è®­ç»ƒçš„ä¸­æ–‡LLaMAå¤§æ¨¡å‹

ğŸš€ å¼€æºäº†è¿›ä¸€æ­¥ç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ä¸­æ–‡Alpacaå¤§æ¨¡å‹

ğŸš€ å¿«é€Ÿåœ°ä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆä¸ªäººPCï¼‰æœ¬åœ°éƒ¨ç½²å’Œä½“éªŒé‡åŒ–ç‰ˆå¤§æ¨¡å‹



**[llama.cpp](https://github.com/ggerganov/llama.cpp)**

Inference of [LLaMA](https://arxiv.org/abs/2302.13971) model in pure C/C++ :weary:

The main goal is to run the model using 4-bit quantization on a MacBook(also supports Windows)

- Plain C/C++ implementation without dependencies
- Apple silicon first-class citizen - optimized via ARM NEON and Accelerate framework
- AVX2 support for x86 architectures
- Mixed F16 / F32 precision
- 4-bit quantization support
- Runs on the CPU



###  Data && Deployment

**[AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned)**

This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.



[Alpaca-LoRA-Serve](https://github.com/deep-diver/Alpaca-LoRA-Serve)

Alpaca-LoRA as a Chatbot Service

<img src="https://cdn.jsdelivr.net/gh/Darren-greenhand/Darren-greenhand-image@main/img/202304011729858.png" alt="img" style="zoom: 19%;" />



**[Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp)**

Run a fast ChatGPT-like model locally on your device.

Based on the [Alpaca-lora](https://github.com/tloen/alpaca-lora) and the horiible [llama.cpp](https://github.com/ggerganov/llama.cpp) but  add a **chat interface**.





## å…è´£å£°æ˜

æœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›å­¦æœ¯ç ”ç©¶ä¹‹ç”¨ï¼Œä¸¥ç¦ç”¨äºå•†ä¸šç”¨é€”ã€‚ä½¿ç”¨æ¶‰åŠç¬¬ä¸‰æ–¹ä»£ç çš„éƒ¨åˆ†æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªç›¸åº”çš„å¼€æºåè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å—æ¨¡å‹è®¡ç®—ã€éšæœºæ€§å’Œé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®æ— æ³•å¯¹å…¶å‡†ç¡®æ€§ä½œå‡ºä¿è¯ã€‚å¯¹äºæ¨¡å‹è¾“å‡ºçš„ä»»ä½•å†…å®¹ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚

æœ¬é¡¹ç›®ç”±ä¸ªäººåŠåä½œè€…ä¸šä½™æ—¶é—´å‘èµ·å¹¶ç»´æŠ¤ï¼Œå› æ­¤æ— æ³•ä¿è¯èƒ½åŠæ—¶å›å¤è§£å†³ç›¸åº”é—®é¢˜ã€‚
